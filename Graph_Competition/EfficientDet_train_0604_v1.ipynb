{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SJin765/class_AI4dl/blob/main/Graph_Competition/EfficientDet_train_0604_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benetech | EfficientDet [Train]\n",
        "\n",
        "https://www.kaggle.com/code/alejopaullier/benetech-efficientdet-train\n",
        "\n",
        "Benetech | Create Bounding Box Dataframe\n",
        "\n",
        "https://www.kaggle.com/code/alejopaullier/benetech-create-bounding-box-dataframe"
      ],
      "metadata": {
        "id": "h4nATJM3hNQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import EfficientDet, Pytorch"
      ],
      "metadata": {
        "id": "S-0ng6X7hfcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocotools"
      ],
      "metadata": {
        "id": "n7i2KEXJht4P",
        "outputId": "95bd2ec1-4cfb-47c0-fe7d-dae2ea6d3003",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.6)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools) (1.22.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "-NQ39PDmMndi",
        "outputId": "ae18b05a-f510-450a-d7a9-8edf5fb0526f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 timm-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aljPIojdNGQD",
        "outputId": "ae7a7aaa-745c-41d4-9969-184e32b46328"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LFVsMVjqhMsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3a988b-4429-4c00-c040-f1836d18defd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 CPUs available\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"../input/timm-efficientdet-pytorch\")\n",
        "sys.path.insert(0, \"../input/omegaconf\")\n",
        "sys.path.insert(0, \"../input/albumentations\")\n",
        "\n",
        "import albumentations as A\n",
        "import copy\n",
        "import cv2\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import random\n",
        "import time\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from colorama import Fore, Back, Style\n",
        "from datetime import datetime, timedelta\n",
        "from glob import glob\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
        "from torch.optim import lr_scheduler\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "c_  = Fore.GREEN\n",
        "sr_ = Style.RESET_ALL\n",
        "print(f\"There are {multiprocessing.cpu_count()} CPUs available\")\n",
        "print()\n",
        "!mkdir logs\n",
        "!mkdir saved_models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "MkvR2N0OjfGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter"
      ],
      "metadata": {
        "id": "HjZPmvl0jomK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 드라이브에 내가 올려놓은 파일을 이용할 경우\n",
        "# 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# file_path = '/content/drive/MyDrive/benetech-making-graphs-accessible'"
      ],
      "metadata": {
        "id": "5m_3fUkAkca9",
        "outputId": "812695db-eaed-4f67-d505-fce3de4d6617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 드라이브 내 압축풀기\n",
        "import zipfile\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/benetech-making-graphs-accessible.zip'\n",
        "extract_path = '/content/drive/MyDrive/benetech-making-graphs-accessible'\n",
        "\n",
        "# 압축 파일 열기\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
        "    # 압축 파일 내 폴더 목록 확인\n",
        "    folder_list = [name for name in zip_file.namelist() if name.endswith('/')]\n",
        "\n",
        "    # 폴더 내용물 확인\n",
        "    for folder_name in folder_list:\n",
        "        print(f\"Folder: {folder_name}\")\n",
        "        file_list = zip_file.namelist()\n",
        "\n",
        "        # 폴더 내 파일 목록 확인\n",
        "        for file_name in file_list:\n",
        "            if file_name.startswith(folder_name) and not file_name.endswith('/'):\n",
        "                print(f\"File: {file_name}\")\n",
        "\n",
        "    # 압축 파일 해제\n",
        "    zip_file.extractall(extract_path)"
      ],
      "metadata": {
        "id": "0b7_7JV9aRlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "    BATCH_SIZE_TRAIN = 4\n",
        "    BATCH_SIZE_VALID = 2\n",
        "    DEBUG = False\n",
        "    DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    EPOCHS = 5\n",
        "    FOLDS = 5\n",
        "    LR = 2e-4\n",
        "    MIN_LR = 1e-6\n",
        "    NUM_WORKERS = multiprocessing.cpu_count()\n",
        "    RESOLUTION = 512\n",
        "    SAMPLE = 30_000\n",
        "    SEED = 42\n",
        "    SCHEDULER = 'CosineAnnealingLR'\n",
        "    T_0 = 25\n",
        "    T_MAX = int(30_000/BATCH_SIZE_TRAIN*EPOCHS)+50\n",
        "    WARMUP_EPOCHS = 0\n",
        "    WEIGHT_DECAY = 1e-6\n",
        "    \n",
        "    \n",
        "class paths:\n",
        "    TRAIN_ANNOTATIONS_FOLDER = \"/content/drive/MyDrive/benetech-making-graphs-accessible/train/annotations/\"\n",
        "    TRAIN_IMAGES_FOLDER = \"/content/drive/MyDrive/benetech-making-graphs-accessible/train/images/\""
      ],
      "metadata": {
        "id": "QEnUANNZh9Ke"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "XvmfDgG3jWo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stoi(df):\n",
        "    \"\"\"Get String to Index dictionary\"\"\"\n",
        "    stoi = {}\n",
        "    for idx, string in enumerate(df.label.unique()):\n",
        "        stoi[string] = idx + 1\n",
        "    itos = {item[1]: item[0] for item in stoi.items()}\n",
        "    df = df.replace({\"label\": stoi})\n",
        "    return stoi\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(config.SEED)"
      ],
      "metadata": {
        "id": "TsowBJJAjYmM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Bounding Box Dataframe"
      ],
      "metadata": {
        "id": "SgUAIoIHfcvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "!mkdir images"
      ],
      "metadata": {
        "id": "lWPtxN0ffdLJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_plot_bb(data):\n",
        "    x0 = data[\"plot-bb\"][\"x0\"]\n",
        "    y0 = data[\"plot-bb\"][\"y0\"]\n",
        "    w = data[\"plot-bb\"][\"width\"]\n",
        "    h = data[\"plot-bb\"][\"height\"]\n",
        "    return [x0, y0, w, h, \"plot-bb\"]\n",
        "\n",
        "\n",
        "def extract_label_bbox(data, rows, img_id):\n",
        "    common = []\n",
        "    common+= [img_id, data[\"source\"], data[\"chart-type\"]]\n",
        "    for box in data[\"text\"]:\n",
        "        x0 = box[\"polygon\"][\"x0\"]\n",
        "        y0 = box[\"polygon\"][\"y0\"]\n",
        "        w = box[\"polygon\"][\"x1\"] - box[\"polygon\"][\"x0\"]\n",
        "        h = box[\"polygon\"][\"y3\"] - box[\"polygon\"][\"y0\"]\n",
        "        label = box[\"role\"]\n",
        "        box_row = common + [x0, y0, w, h, label]\n",
        "        rows.append(box_row)\n",
        "    rows.append(common + extract_plot_bb(data)) \n",
        "    return rows\n",
        "        \n",
        "def extract_tick_bbox(data, rows, img_id):\n",
        "    common = []\n",
        "    common+= [img_id , data[\"source\"], data[\"chart-type\"]]\n",
        "    for axis in data[\"axes\"].keys():\n",
        "        for box in data[\"axes\"][axis][\"ticks\"]:\n",
        "            x0 = box[\"tick_pt\"][\"x\"] - 5\n",
        "            y0 = box[\"tick_pt\"][\"y\"] - 5\n",
        "            w = 10\n",
        "            h = 10\n",
        "            label = axis + \"-tick\"\n",
        "            box_row = common + [x0, y0, w, h, label]\n",
        "            rows.append(box_row)\n",
        "    return rows"
      ],
      "metadata": {
        "id": "zCp-GD8yf_4p"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 자신의 train 데이터 annotations 폴더 지정\n",
        "ANNOTATION = \"/content/drive/MyDrive/benetech-making-graphs-accessible/train/annotations/*.json\"\n",
        "rows = []\n",
        "for file_name in tqdm(glob(ANNOTATION)):\n",
        "    label_bbox = []\n",
        "    tick_bbox = []\n",
        "    with open(file_name) as f:\n",
        "        data = json.load(f)\n",
        "        img_id = file_name.split(\"/\")[-1].split(\".\")[0]\n",
        "        label_bbox = extract_label_bbox(data, label_bbox, img_id)\n",
        "        tick_bbox = extract_tick_bbox(data, tick_bbox, img_id)\n",
        "        data_rows = label_bbox + tick_bbox\n",
        "    rows += data_rows"
      ],
      "metadata": {
        "id": "MUt4aPj6gC55",
        "outputId": "9099585d-4940-49d5-fc1e-9e788ea783b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 2119/60578 [09:04<3:39:46,  4.43it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(rows)\n",
        "df.columns = [\"image_id\", \"source\", \"chart\", \"x0\", \"y0\", \"w\", \"h\", \"label\"]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9dTq21wsgWsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bounding box 시각화\n",
        "import matplotlib.patches as patches\n",
        "fig, ax = plt.subplots()\n",
        "plt.rcParams[\"figure.figsize\"] = (20,15)\n",
        "image_id = random.choice(df[\"image_id\"].unique().tolist())\n",
        "sample = df[df[\"image_id\"]==image_id]\n",
        "for index, row in sample.iterrows():\n",
        "    x0, y0, W, H = row[\"x0\"], row[\"y0\"], row[\"w\"], row[\"h\"]\n",
        "    rect = patches.Rectangle((x0, y0), W, H,\n",
        "                             linewidth=1,\n",
        "                             edgecolor='g',\n",
        "                             facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "TRAIN_FOLDER = \"/content/drive/MyDrive/benetech-making-graphs-accessible/train/images/\"\n",
        "image = cv2.imread(TRAIN_FOLDER + image_id + \".jpg\")\n",
        "\n",
        "# Display the image\n",
        "ax.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mo3ezcsTgaZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/benetech-making-graphs-accessible/train/train.csv\", index=False)"
      ],
      "metadata": {
        "id": "5eBMEH1xgcQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Competition Data"
      ],
      "metadata": {
        "id": "g7SeKj6ih71-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/benetech-making-graphs-accessible/train/train.csv\")\n",
        "print(f\"Dataframe shape is: {df.shape}\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "sMz9AYHJkyJM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "e853ace6-f0b7-4d7c-f9f2-283c8713d610"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-51e5992f9415>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df = pd.read_csv(\"/kaggle/input/benetech-create-bounding-box-dataframe/train.csv)\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-procesing"
      ],
      "metadata": {
        "id": "6--B7BCCk8Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_ids = df[\"image_id\"].unique().tolist()\n",
        "image_ids = random.sample(image_ids, config.SAMPLE)\n",
        "filter = df[(df[\"x0\"]<0) | (df[\"y0\"]<0) | (df[\"h\"]<=0) | (df[\"w\"]<=0)].index\n",
        "df = df[~df.index.isin(filter)]\n",
        "df = df[df[\"image_id\"].isin(image_ids)]\n",
        "stoi = get_stoi(df)\n",
        "pprint(stoi)\n",
        "config.NUM_CLASSES =  len(stoi)\n",
        "df = df[df[\"label\"].isin(list(stoi.keys()))]\n",
        "df = df.replace({\"label\": stoi})\n",
        "df.reset_index(inplace=True)\n",
        "print(f\"Dataframe shape is: {df.shape}\")\n",
        "print(f\"Number of classes: {config.NUM_CLASSES}\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "02VZAKCIk9qB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "bc5c7a46-6adf-4676-c8a3-ef0b79b679e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-973cae0fa266>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimage_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAMPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"x0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"h\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split into train, validation"
      ],
      "metadata": {
        "id": "er596wHblLBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gkf = GroupKFold(n_splits=config.FOLDS) # Seed for reproducibility\n",
        "X = df.loc[:, df.columns != \"label\"]\n",
        "y = df.loc[:, df.columns == \"label\"]\n",
        "groups = df.loc[:, df.columns == \"image_id\"]\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(gkf.split(X, y, groups)):\n",
        "    df.loc[val_index, 'fold'] = int(fold) # Assign to each row its Fold ID\n",
        "display(df.groupby(['fold','chart'])[\"index\"].count())"
      ],
      "metadata": {
        "id": "aK-45J8WlO35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Albumentations transformations\n",
        "\n"
      ],
      "metadata": {
        "id": "64W06qpllUap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_transforms():\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.Resize(height=config.RESOLUTION, width=config.RESOLUTION, p=1),\n",
        "            A.Normalize(p=1),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ], \n",
        "        p=1.0, \n",
        "        bbox_params=A.BboxParams(\n",
        "            format='pascal_voc', min_area=0,  min_visibility=0, label_fields=['labels']\n",
        "        )\n",
        "    )\n",
        "\n",
        "def get_valid_transforms():\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.Resize(height=config.RESOLUTION, width=config.RESOLUTION, p=1.0),\n",
        "            A.Normalize(p=1),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ], \n",
        "        p=1.0, \n",
        "        bbox_params=A.BboxParams(\n",
        "            format='pascal_voc', min_area=0, min_visibility=0, label_fields=['labels']\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "I2s9oQDmlUBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create custom PyTorch dataset"
      ],
      "metadata": {
        "id": "cgKHdh7RlhXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, transforms=None):\n",
        "        super().__init__()\n",
        "        self.df = df # pandas dataframe\n",
        "        self.transforms = transforms # albumentations transformations\n",
        "        self.image_ids = self.df[\"image_id\"].unique().tolist() # list with unique image IDs\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        \"\"\"\n",
        "        :return image: an augmented image as a numpy array.\n",
        "        :return target: a dictionary containing a tensor with the bboxes for the image (torch.Tensor),\n",
        "        a list of strings containing the bboxes labels and a tensor containing the image index.\n",
        "        :return image_id: the image ID. A unique identifier for each image. \n",
        "        \n",
        "        \"\"\"\n",
        "        image_id = self.image_ids[index] # select one image\n",
        "        image, boxes = self.load_image_and_boxes(index) # load the image and its associated bounding boxes\n",
        "        labels = self.get_labels(index)\n",
        "        target = {\n",
        "            'boxes' : boxes, \n",
        "            'labels' : labels, \n",
        "            'index' : torch.tensor([index])\n",
        "        }\n",
        "        transformed_image = self.transforms(**{\n",
        "            'image': image,\n",
        "            'bboxes': target['boxes'],\n",
        "            'labels': target[\"labels\"]\n",
        "        })\n",
        "        image = transformed_image['image']\n",
        "        _, new_h, new_w = image.shape\n",
        "        \n",
        "        # This creates a torch tensor of size (number_bboxes, 4) where each row is a bounding box:\n",
        "        target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*transformed_image['bboxes'])))).permute(1, 0)\n",
        "        target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  # Required: change order to: (y, x, y, x)\n",
        "        target[\"img_size\"] = (new_h, new_w)\n",
        "        target[\"img_scale\"] = torch.tensor([1.0])\n",
        "        return image, target, image_id\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def load_image_and_boxes(self, index):\n",
        "        \"\"\"\n",
        "        :return image: the image as a numpy array. The array is scaled to the [0,1] interval. Numpy array.\n",
        "        :return boxes: an array containing bounding boxes rows = [x0, y0, x0 + height, y0 + width]. List of lists.\n",
        "        \"\"\"\n",
        "        image_id = self.image_ids[index] # select image\n",
        "        image = cv2.imread(f'{paths.TRAIN_IMAGES_FOLDER}{image_id}.jpg', cv2.IMREAD_COLOR) # read image from path\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32) # convert to RGB \n",
        "        records = self.df[self.df['image_id'] == image_id] # select all rows corresponding to the image\n",
        "        boxes = records[['x0', 'y0', 'w', 'h']].values # get bounding box information\n",
        "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2] # x0 + Δx (or also  x0 + height), pascal_voc format\n",
        "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3] # y0 + Δy (or also  y0 + width), pascal_voc format\n",
        "        boxes = boxes.tolist() # convert to list\n",
        "        return image, boxes\n",
        "    \n",
        "    def get_labels(self, index):\n",
        "        image_id = self.image_ids[index]\n",
        "        labels = self.df[self.df['image_id'] == image_id][\"label\"].values.tolist()\n",
        "        labels = torch.tensor(labels)\n",
        "        return labels"
      ],
      "metadata": {
        "id": "vipb5BtFlnqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare dataloaders"
      ],
      "metadata": {
        "id": "oGj0x6e2lukL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "def prepare_loaders(fold, df):\n",
        "    \"\"\"\n",
        "    Splits data into Train and Validation sets depending on the current fold. Creates PyTorch\n",
        "    datasets from the splits. Creates DataLoaders from Datasets.\n",
        "    :param fold: current fold (int).\n",
        "    :param df: train dataframe (pandas dataframe).\n",
        "    :return train_loader, valid_loader: dataloaders for each stage.\n",
        "    \"\"\"\n",
        "    # === Select data for Train and Validation ===\n",
        "    train_df = df.query(\"fold!=@fold\").reset_index(drop=True) # Select all rows not from validation fold\n",
        "    valid_df = df.query(\"fold==@fold\").reset_index(drop=True) # Select all rows from validation fold\n",
        "    \n",
        "    # === Mini sample for debugging purposes ===\n",
        "    if config.DEBUG:\n",
        "        train_df = train_df.head(32*5).query(\"empty==0\")\n",
        "        valid_df = valid_df.head(32*3).query(\"empty==0\")\n",
        "    \n",
        "    # === Build Datasets ===\n",
        "    train_dataset = CustomDataset(train_df, transforms=get_train_transforms())\n",
        "    valid_dataset = CustomDataset(valid_df, transforms=get_valid_transforms())\n",
        "    \n",
        "    # === Create DataLoaders for Train and Validation ===\n",
        "    train_loader = DataLoader(train_dataset, \n",
        "                              batch_size=config.BATCH_SIZE_TRAIN if not config.DEBUG else 20, \n",
        "                              num_workers=config.NUM_WORKERS,\n",
        "                              sampler=RandomSampler(train_dataset),\n",
        "                              pin_memory=False, drop_last=False, collate_fn=collate_fn)\n",
        "    valid_loader = DataLoader(valid_dataset,\n",
        "                              batch_size=config.BATCH_SIZE_VALID if not config.DEBUG else 20, \n",
        "                              num_workers=config.NUM_WORKERS,\n",
        "                              sampler=SequentialSampler(valid_dataset),\n",
        "                              shuffle=False, pin_memory=True, collate_fn=collate_fn)\n",
        "    \n",
        "    return train_loader, valid_loader\n",
        "\n",
        "\n",
        "train_loader, valid_loader = prepare_loaders(fold=0, df=df)"
      ],
      "metadata": {
        "id": "Vzal03pUluTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scheduler"
      ],
      "metadata": {
        "id": "Xsyf3qitl9AY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_scheduler(optimizer):\n",
        "    if config.SCHEDULER == 'CosineAnnealingLR':\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=config.T_MAX, \n",
        "                                                   eta_min=config.MIN_LR)\n",
        "    elif config.SCHEDULER == 'CosineAnnealingWarmRestarts':\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=config.T_0, \n",
        "                                                             eta_min=config.MIN_LR)\n",
        "    elif config.SCHEDULER == 'ReduceLROnPlateau':\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                   mode='min',\n",
        "                                                   factor=0.1,\n",
        "                                                   patience=7,\n",
        "                                                   threshold=0.0001,\n",
        "                                                   min_lr=config.MIN_LR,)\n",
        "    elif config.SCHEDULER == 'ExponentialLR':\n",
        "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n",
        "    elif config.SCHEDULER == None:\n",
        "        return None\n",
        "        \n",
        "    return scheduler"
      ],
      "metadata": {
        "id": "lp7-3lHTl8xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "6WBBo3cVmIT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create training Model"
      ],
      "metadata": {
        "id": "4vtrwEJnmMoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from effdet.config.model_config import efficientdet_model_param_dict\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n",
        "from effdet.efficientdet import HeadNet\n",
        "from effdet.config.model_config import efficientdet_model_param_dict\n",
        "\n",
        "def create_model(num_classes=config.NUM_CLASSES, image_size=512,\n",
        "                 architecture=\"tf_efficientnetv2_s\", verbose=False):\n",
        "    \n",
        "    efficientdet_model_param_dict['tf_efficientnetv2_s'] = dict(\n",
        "        name='tf_efficientnetv2_s',\n",
        "        backbone_name='tf_efficientnetv2_s',\n",
        "        backbone_args=dict(drop_path_rate=0.2),\n",
        "        num_classes=num_classes,\n",
        "        url='')\n",
        "    \n",
        "    cfg = get_efficientdet_config(architecture)\n",
        "    cfg.update({'num_classes': num_classes})\n",
        "    cfg.update({'image_size': (image_size, image_size)})\n",
        "    \n",
        "    if verbose:\n",
        "        pprint(cfg)\n",
        "\n",
        "    net = EfficientDet(cfg, pretrained_backbone=True)\n",
        "    net.class_net = HeadNet(\n",
        "        cfg,\n",
        "        num_outputs=cfg.num_classes,\n",
        "    )\n",
        "    return DetBenchTrain(net, cfg)\n",
        "\n",
        "\n",
        "def load_model(model_weights_path, model):\n",
        "    \"\"\"\n",
        "    Load model weights.\n",
        "    \"\"\"\n",
        "    model.load_state_dict(torch.load(model_weights_path))\n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "f5jHZOLfmMPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average Meter : Track metrics and loss"
      ],
      "metadata": {
        "id": "nMJVuebgmbLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Initializes an instance by reseting its values\"\"\"\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets all values to zero\"\"\"\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        \"\"\"\n",
        "        Tracks values, count, sum and average.\n",
        "        :param val: usually the loss function value.\n",
        "        :param n: usually the number of samples.\n",
        "        \"\"\"\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "po1gWiEzmnN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Function : 1 epoch"
      ],
      "metadata": {
        "id": "Stabp9i_mhc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
        "    model.train() # Set model in training mode\n",
        "    loss_meter = AverageMeter() # Create instance\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ') # Progress bar\n",
        "    for step, (images, targets, image_ids) in pbar:\n",
        "        # === Collate ===\n",
        "        images = torch.stack(images).to(device).float() # Get images (batch_size, 3, RESOLUTION, RESOLUTION)\n",
        "        batch_size = images.shape[0] # Get batch size\n",
        "        boxes = [target['boxes'].to(device).float() for target in targets] # Get bounding boxes\n",
        "        labels = [target['labels'].to(device).float() for target in targets] # Get labels (tuple with strings)\n",
        "        img_size = torch.tensor([target[\"img_size\"] for target in targets]).to(device).float()\n",
        "        img_scale = torch.tensor([target[\"img_scale\"] for target in targets]).to(device).float()\n",
        "        annotations = {\n",
        "            \"bbox\": boxes,\n",
        "            \"cls\": labels,\n",
        "            \"img_size\": img_size,\n",
        "            \"img_scale\": img_scale\n",
        "        }\n",
        "        optimizer.zero_grad() # Zero out gradients\n",
        "        loss = model(images, annotations) # Forward pass\n",
        "        loss = loss[\"loss\"]\n",
        "        loss.backward() # Back propagation\n",
        "        # Since the reduction type of the loss is \"mean\" we multiply by batch_size\n",
        "        loss_meter.update(loss.detach().item(), batch_size) # Update loss\n",
        "        optimizer.step() # Update params\n",
        "        scheduler.step() # Update learning rate\n",
        "        \n",
        "        # === Evaluate model ===\n",
        "        \n",
        "        mem = torch.cuda.memory_reserved() / 1e9 if torch.cuda.is_available() else 0 # Track memory\n",
        "        current_lr = optimizer.param_groups[0]['lr'] # Get current Learning Rate\n",
        "        pbar.set_postfix(train_loss=f'{loss_meter.avg:0.4f}',\n",
        "                         lr=f'{current_lr:0.5f}',\n",
        "                         gpu_mem=f'{mem:0.2f} GB')\n",
        "    # === Release memory ===\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    return loss_meter"
      ],
      "metadata": {
        "id": "CR1qqLbemoLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Function : 1 epoch"
      ],
      "metadata": {
        "id": "sdBulxBxmvr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def valid_one_epoch(model, dataloader, device, epoch):\n",
        "    model.eval() # Set model in evaluation mode\n",
        "    loss_meter = AverageMeter() # Create instance\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ') # Progress bar\n",
        "    for step, (images, targets, image_ids) in pbar:  \n",
        "        # === Collate ===\n",
        "        images = torch.stack(images).to(device).float() # Get images\n",
        "        batch_size = images.shape[0] # Get batch size\n",
        "        boxes = [target['boxes'].to(device).float() for target in targets] # Get boxes\n",
        "        labels = [target['labels'].to(device).float() for target in targets] # Get labels\n",
        "        img_size = torch.tensor([target[\"img_size\"] for target in targets]).to(device).float()\n",
        "        img_scale = torch.tensor([target[\"img_scale\"] for target in targets]).to(device).float()\n",
        "        \n",
        "        annotations = {\n",
        "            \"bbox\": boxes,\n",
        "            \"cls\": labels,\n",
        "            \"img_size\": img_size,\n",
        "            \"img_scale\": img_scale\n",
        "        }\n",
        "        loss = model(images, annotations) # Forward pass\n",
        "        loss = loss[\"loss\"]\n",
        "        loss_meter.update(loss.detach().item(), batch_size) # Update loss\n",
        "        # === Evaluate model ===\n",
        "        \n",
        "        mem = torch.cuda.memory_reserved() / 1e9 if torch.cuda.is_available() else 0 # Track memory\n",
        "        current_lr = optimizer.param_groups[0]['lr'] # Get current learning rate\n",
        "        pbar.set_postfix(valid_loss=f'{loss_meter.avg:0.4f}',\n",
        "                         lr=f'{current_lr:0.5f}',\n",
        "                         gpu_memory=f'{mem:0.2f} GB')\n",
        "    # === Release memory ===\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    return loss_meter"
      ],
      "metadata": {
        "id": "o1_cGVBQmumP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Loop"
      ],
      "metadata": {
        "id": "nHuXHHAZm9YG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, optimizer, scheduler, device, num_epochs):\n",
        "    f = open(f\"/kaggle/working/logs/logs.txt\", \"w+\") # Create log file\n",
        "    \n",
        "    if torch.cuda.is_available(): # Check if GPU is available\n",
        "        print(\"Cuda: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "    \n",
        "    start = time.time() # Track execution time\n",
        "    best_model_weights = copy.deepcopy(model.state_dict())\n",
        "    epochs = config.EPOCHS\n",
        "    best_loss = 1e10\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f'Epoch {epoch}/{num_epochs}', end='')\n",
        "        loss_meter_train = train_one_epoch(model, optimizer, scheduler, \n",
        "                                           dataloader=train_loader, \n",
        "                                           device=config.DEVICE, epoch=epoch)\n",
        "        \n",
        "        loss_meter_valid = valid_one_epoch(model, valid_loader, \n",
        "                                           device=config.DEVICE, \n",
        "                                           epoch=epoch)\n",
        "        \n",
        "        duration = str(timedelta(seconds=time.time() - start))[:7]\n",
        "        # === Print to log file ===\n",
        "        with open(f\"logs/logs.txt\", 'a+') as f:\n",
        "            print('{} | Epoch: {}/{} | Train Loss: {:.4} '. \\\n",
        "            format(duration, epoch + 1, epochs, loss_meter_train.avg), file=f)\n",
        "            print('{} | Epoch: {}/{} | Valid Loss: {:.4}'. \\\n",
        "            format(duration, epoch + 1, epochs, loss_meter_valid.avg), file=f)\n",
        "            print(\"\\n\" + \"-\"*100 + \"\\n\", file=f)\n",
        "        \n",
        "        # === Save model if there is an improvement ===\n",
        "        if loss_meter_valid.avg < best_loss:\n",
        "            best_loss = loss_meter_valid.avg\n",
        "            best_epoch = epoch\n",
        "            best_model_weights = copy.deepcopy(model.state_dict())\n",
        "            PATH = f\"/kaggle/working/saved_models/best_epoch-{fold:02d}.bin\"\n",
        "            torch.save(model.state_dict(), PATH)\n",
        "            print(f\"Model Saved | Best Epoch {best_epoch} | Best Loss {round(best_loss,2)} {sr_}\")\n",
        "            \n",
        "        last_model_wts = copy.deepcopy(model.state_dict())\n",
        "        PATH = f\"last_epoch-{fold:02d}.bin\"\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "        print(); print()\n",
        "    \n",
        "    end = time.time()\n",
        "    time_elapsed = end - start\n",
        "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "PiQYZ2K0m8sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "fIV2JRdYnAed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fold in range(1):\n",
        "    print(f'================= Fold: {1} =================')\n",
        "    train_loader, valid_loader = prepare_loaders(fold=fold, df=df)\n",
        "    model = create_model()\n",
        "    model.to(config.DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n",
        "    scheduler = fetch_scheduler(optimizer)\n",
        "    model = train_loop(model, optimizer, scheduler,\n",
        "                       device=config.DEVICE,\n",
        "                       num_epochs=config.EPOCHS)"
      ],
      "metadata": {
        "id": "yIrIMtXBnAIE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}